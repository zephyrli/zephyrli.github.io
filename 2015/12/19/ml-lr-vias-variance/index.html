<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>机器学习第一章--逻辑回归，Bias和Variance | Zephyr&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="首先从机器学习的基本概念说起。
经验风险（Experiencal Risk)机器学习本质上是一种对问题真实模型的逼近，这种近似模型也叫作一个假设。假设肯定与真实模型之间存在有误差，这种误差或者误差的积累也被称为“风险”(risk)。
在我们选择了一个假设后，为了得到真实误差的逼近，我们用模型在样本数据上的分类结果与样本本身真实结果之间的差值来表示。这个差值叫做经验风险。
经验风险并不能很好的体现模">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习第一章--逻辑回归，Bias和Variance">
<meta property="og:url" content="http://yoursite.com/2015/12/19/ml-lr-vias-variance/index.html">
<meta property="og:site_name" content="Zephyr's blog">
<meta property="og:description" content="首先从机器学习的基本概念说起。
经验风险（Experiencal Risk)机器学习本质上是一种对问题真实模型的逼近，这种近似模型也叫作一个假设。假设肯定与真实模型之间存在有误差，这种误差或者误差的积累也被称为“风险”(risk)。
在我们选择了一个假设后，为了得到真实误差的逼近，我们用模型在样本数据上的分类结果与样本本身真实结果之间的差值来表示。这个差值叫做经验风险。
经验风险并不能很好的体现模">
<meta property="og:image" content="https://raw.githubusercontent.com/zephyrli/pics/master/Bias_Variance.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/zephyrli/pics/master/equation%20(1).png">
<meta property="og:image" content="https://raw.githubusercontent.com/zephyrli/pics/master/1degree_4degree.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zephyrli/pics/master/goodfit_overfitting.png">
<meta property="og:updated_time" content="2015-12-21T03:04:08.906Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习第一章--逻辑回归，Bias和Variance">
<meta name="twitter:description" content="首先从机器学习的基本概念说起。
经验风险（Experiencal Risk)机器学习本质上是一种对问题真实模型的逼近，这种近似模型也叫作一个假设。假设肯定与真实模型之间存在有误差，这种误差或者误差的积累也被称为“风险”(risk)。
在我们选择了一个假设后，为了得到真实误差的逼近，我们用模型在样本数据上的分类结果与样本本身真实结果之间的差值来表示。这个差值叫做经验风险。
经验风险并不能很好的体现模">
  
    <link rel="alternative" href="/atom.xml" title="Zephyr&#39;s blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zephyr&#39;s blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Don&#39;t stop to be stronger</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-ml-lr-vias-variance" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/12/19/ml-lr-vias-variance/" class="article-date">
  <time datetime="2015-12-19T08:17:25.000Z" itemprop="datePublished">2015-12-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习第一章--逻辑回归，Bias和Variance
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>首先从机器学习的基本概念说起。</p>
<h3 id="u7ECF_u9A8C_u98CE_u9669_uFF08Experiencal_Risk_29"><a href="#u7ECF_u9A8C_u98CE_u9669_uFF08Experiencal_Risk_29" class="headerlink" title="经验风险（Experiencal Risk)"></a>经验风险（Experiencal Risk)</h3><p><strong>机器学习本质上是一种对问题真实模型的逼近</strong>，这种近似模型也叫作一个假设。假设肯定与真实模型之间存在有误差，这种误差或者误差的积累也被称为“风险”(risk)。</p>
<p>在我们选择了一个假设后，为了得到真实误差的逼近，我们用<strong>模型在样本数据上的分类结果与样本本身真实结果之间的差值</strong>来表示。这个差值叫做<strong>经验风险</strong>。</p>
<p>经验风险并不能很好的体现模型的优劣，因为很多分类函数在样本集合上能够很轻易的获得很高正确率,但是对真实数据的预测却很糟。也表明了这种模型的推广能力（泛化能力）差。</p>
<p>导致这种现象的原因是：经验风险并不能够真正的逼近真实风险,因为样本集合的数目相对于真实世界的数据来说始终是很小的。之后统计学中就引入了泛化误差界的概念。</p>
<h3 id="u6CDB_u5316_u8BEF_u5DEE_28Generalization_error_29"><a href="#u6CDB_u5316_u8BEF_u5DEE_28Generalization_error_29" class="headerlink" title="泛化误差(Generalization error)"></a>泛化误差(Generalization error)</h3><p>泛化误差刻画了学习算法的经验风险与期望风险之间偏差和收敛速度.在机器学习中泛化误差是用来衡量一个学习机器推广未知数据的能力，即根据从样本数据中学习到的规则能够应用到新数据的能力。常用的计算方法是：用在训练集上的误差平均值-在测试集上的误差平均值。</p>
<p>真实的风险应该由两部分组成：</p>
<p>1：经验风险,代表分类器在给定样本上的误差（可以精确计算）。     </p>
<p>2：置信风险,代表我们可以在多大程度上信任分类器在未知数据上的分类结果（不可以精确计算）,因为不可以精确计算,所以只能给出一个估计区间,也因为这个泛化误差只能给出一个上界。 与置信风险相关的变量有两个：</p>
<p>a)样本数量,样本数量越大表明我们的学习结果正确的可能性越大,此时置信风险越小。</p>
<p>b)VC维,分类函数的VC维越大,推广能力越差,置信风险越大。</p>
<p>真实风险 ≤ 经验风险 + 置信风险。</p>
<p>现在统计学习的目标就从经验风险最小化变为经验风险与置信风险之和最小化。</p>
<h2 id="Bias_2C_Variance"><a href="#Bias_2C_Variance" class="headerlink" title="Bias, Variance"></a>Bias, Variance</h2><p>在A Few Useful Thingsto Know about Machine Learning中提到，可以将泛化误差（gener-alization error）分解成bias和variance理解。</p>
<p><strong>Bias</strong>: a learner’s tendency to consistently learn the same wrong thing，即度量了某种学习算法的平均估计结果所能逼近学习目标(目标输出)的程度。简单来说，bias代表了模型预测与结果的误差。</p>
<p><strong>Variance</strong>：the tendency to learn random things irrespective of the real signal，即度量了在面对同样规模的不同训练集时，学习算法的估计结果发生变动的程度。简单说，variance代表了模型对于训练数据的敏感性。</p>
<p>可以用下面这个图来阐述两者的概念。</p>
<p><img src="https://raw.githubusercontent.com/zephyrli/pics/master/Bias_Variance.jpg" alt="Bias and Variance"></p>
<p>靶心为某个能完美预测的模型，离靶心越远则准确率越低。靶上的点代表某次对某个数据集上学习某个模型。从图上可以看出，高的bias表示预测离目标较远，低bias表示离靶心越近；高的variance表示在不同训练集上的多次学习结果越分散，反之越集中。</p>
<p>同样我们可以看出，高的bias或者高的variance都会让一个模型偏离目标很多。一般来说，关于bias和variance都会有一个tradeoff，即，有高bias的模型可以达到比较低的variance，有低的bias的模型会带来比较高的variance。</p>
<h2 id="Overfitting_and_underfitting"><a href="#Overfitting_and_underfitting" class="headerlink" title="Overfitting and underfitting"></a>Overfitting and underfitting</h2><p>无论是高的bias还是高的variance对于模型都是不好的性质。当我们的模型有high bias的时候，我们称它“underfits”数据；当我们的模型有high variance和low bias的时候，我们称它”overfits”数据。</p>
<p>我们在这一章将会解释一下为什么会如此。</p>
<p>假设我们有一组训练数据有n个数据对 (x1, y1), (x2, y2), …. (xn, yn). 我们的目标是选择一个方程来在给到x的情况下预测y的值。所以我们希望找到一个方程f(x)逼近训练数据(我们希望误差 | f(xi) – yi | 对于每个训练集数据对都比较小)。这种学习问题被称为回归(regression)。</p>
<p>回归问题和分类问题不同。分类问题中，我们只希望去预测一个离散的标签。在回归问题中，我们希望预测一个值(y)。Overfitting和Underfitting问题同样适用于分类问题，但是在回归问题中比较容易视觉化这个case。</p>
<p>假设我们想用某个阶的多项式来表示f(x)。假设我们选择了d阶多项式，那我们有一组可能的模型去选择，可以用下面的形式来表示这组模型，其中，a_i是不同项的参数。</p>
<p><img src="https://raw.githubusercontent.com/zephyrli/pics/master/equation%20(1).png" alt=""></p>
<p>阶数越高，表达式越复杂。一阶多项式可以用一条直线表示，而阶多项式是抛物线，等等。</p>
<p>我们用三个不同阶数的模型来拟合我们的数据集：一阶多项式，二阶多项式和五阶多项式。分别如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/zephyrli/pics/master/1degree_4degree.png" alt="equation2"></p>
<p>最左边的是一阶，中间是二阶，最后边是五阶。</p>
<p>最右边的模型完美的拟合了数据集，所以它没有bias。事实上，任何至少n-1阶的多项式可以完美拟合一个大小为n的数据集（只要满足yi = yj if xi = xj)。然而当我们去观察这条曲线时，会发现它的行为是不稳定的，它对于最左边的一个点的处理似乎和数据整体的模式很不相同。它为了拟合每一个点做了太多，已经overfit了数据。如果我们需要用它去做一个预测，看起来不太能给给出一个精确的预测，尤其是在x &lt;　１的情况下。</p>
<p>如果我们轻微的移动一个数据点，为了拟合每一个点这个曲线的变化会是非常巨大的。所以这个模型有low bias和high variance。</p>
<p>另一方面，一维多项式有high bias，因为模型不够灵活。无论数据集有多么剧烈的抖动，我们始终只能画一条直线，它underfits了数据。但是如果我们轻微移动了某个数据点，这条直线的变化是非常小的，所以这个模型的variance很小。</p>
<p>在这个例子中，中间的曲线拟合的结果非常不错，没有明显的overfits或是underfits数据。</p>
<p>Overfitting在机器学习中往往是一个比underfitting更重要的问题。经常是在数据中有噪声（小的，随机的错误）的情况下，过拟合数据会导致模型拟合了噪声。有低variance的模型在有噪声的数据上会更加鲁邦。</p>
<p><img src="https://raw.githubusercontent.com/zephyrli/pics/master/goodfit_overfitting.png" alt="goodfits-overfitting"></p>
<p>在上面的图中，右边的模型拟合了数据集中的噪声，左边的模型对于噪声比较鲁邦。</p>
<h2 id="u6211_u4EEC_u5982_u4F55_u77E5_u9053_u662F_u5426_u8FC7_u62DF_u5408_u4E86_u6570_u636E_uFF1F"><a href="#u6211_u4EEC_u5982_u4F55_u77E5_u9053_u662F_u5426_u8FC7_u62DF_u5408_u4E86_u6570_u636E_uFF1F" class="headerlink" title="我们如何知道是否过拟合了数据？"></a>我们如何知道是否过拟合了数据？</h2><p>我们选择越复杂的模型，在数据集上的误差会越小。正如上面的例子，我们选择足够复杂的模型后，在训练集上的误差可以达到零。</p>
<p>有一些方法可以减少过拟合，我们以后的章节再探讨~</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/12/19/ml-lr-vias-variance/" data-id="ciifdjmaf00033ss5huxhe4uj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习，-逻辑回归，-bias-variance/">机器学习， 逻辑回归， bias, variance</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/12/14/my-new-post/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">ror + mongoid开发初探</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ror-mongoid-rails-api/">ror, mongoid, rails api</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习，-逻辑回归，-bias-variance/">机器学习， 逻辑回归， bias, variance</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ror-mongoid-rails-api/" style="font-size: 10px;">ror, mongoid, rails api</a> <a href="/tags/机器学习，-逻辑回归，-bias-variance/" style="font-size: 10px;">机器学习， 逻辑回归， bias, variance</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">十二月 2015</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/12/19/ml-lr-vias-variance/">机器学习第一章--逻辑回归，Bias和Variance</a>
          </li>
        
          <li>
            <a href="/2015/12/14/my-new-post/">ror + mongoid开发初探</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 zephyrli<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>